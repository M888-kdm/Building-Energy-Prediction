{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from pycaret import regression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(os.path.join(str(Path.cwd().parent), \"src\"))\n",
    "from settings.params import *\n",
    "from metrics import eval_metrics\n",
    "from pipeline import define_pipeline\n",
    "from plot import prediction_error_plot, residual_plot\n",
    "from tracking import mlflow_log_search\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CLEANED_DATA)\n",
    "TARGET_NAME = MODEL_PARAMS['TARGET_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, data[TARGET_NAME], test_size=MODEL_PARAMS[\"TEST_SIZE\"], random_state=SEED)\n",
    "\n",
    "logger.info(f\"\\nX train: {x_train.shape}\\nY train: {y_train.shape}\\n\"\n",
    "            f\"X test: {x_test.shape}\\nY test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_train.copy()\n",
    "df[TARGET_NAME] = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_reg = regression.setup(df, target=TARGET_NAME, max_encoding_ohe=200, log_experiment=True, experiment_name=\"building-energy-prediction-training\", train_size=0.8)\n",
    "regression.set_config('seed', SEED)\n",
    "\n",
    "# Removing useless metrics improve training speed\n",
    "regression.remove_metric('MAPE')\n",
    "regression.remove_metric('MSE')\n",
    "regression.remove_metric('RMSLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threes_model = regression.compare_models(n_select=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrainement sur plusieurs types de modèles avec PyCaret montre que pour ce problème, les modèles: ExtraTreesRegressor, XGBRegressor et RandomForestRegressor sont les plus adaptés. Toutefois, nous pensons que les performances obtenus lors de ce premier entrainement peuvent être nettement améliorées. Nous allons attendre de faire le réglage de paramètres et d'obtenir les modèles finaux avant de faire un choix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATOR_PARAMS = {\n",
    "    ExtraTreesRegressor.__name__: {\n",
    "        \"estimator\": ExtraTreesRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        \"estimator\": RandomForestRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    },\n",
    "    xgb.XGBRegressor.__name__: {\n",
    "        \"estimator\": xgb.XGBRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = pendulum.now()\n",
    "\n",
    "search_cvs = {}\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "scoring = {'r2': make_scorer(r2_score),\n",
    "          'rmse': make_scorer(rmse, greater_is_better=False),\n",
    "          'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
    "\n",
    "# Create an experiment if not exists\n",
    "exp_name = \"building-energy-prediction-tuning-sklearn\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Session-{CURRENT_DATE.strftime('%Y%m%d_%H%m%S')}\", experiment_id=experiment_id) as parent_run:\n",
    "    for estimator_name, settings in ESTIMATOR_PARAMS.items():\n",
    "        with mlflow.start_run(run_name=estimator_name, nested=True, experiment_id=experiment_id):  \n",
    "            estimator = settings[\"estimator\"]\n",
    "            param_grid = settings[\"params\"]\n",
    "            pipeline = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"), RobustScaler()],\n",
    "                            categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"), OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                            target_transformer=True,\n",
    "                            estimator=estimator\n",
    "                        ) \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,  # Instantiate the estimator\n",
    "                param_grid=param_grid,\n",
    "                scoring=scoring,\n",
    "                refit='r2',\n",
    "                cv=5,  # Adjust the number of cross-validation folds as needed\n",
    "                n_jobs=-1  # Use all available cores\n",
    "            )\n",
    "            grid_search.fit(x_train, y_train)\n",
    "            search_cvs[estimator_name] = grid_search\n",
    "\n",
    "            mlflow.log_param(\"Estimator\", estimator_name)\n",
    "            mlflow_log_search(grid_search)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to evaluate the fine-tuned models to see which one we are going to pick as the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining best_models after fine-tuning\n",
    "models = { f\"{estimator_name}\": search_cv.best_estimator_ for estimator_name, search_cv in search_cvs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"building-energy-prediction-evaluation\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "def evaluate_models(estimators, x_train, x_test, y_train, y_test):\n",
    "    # Dict of R2 scores for the estimators\n",
    "    r2_scores = {}\n",
    "    with mlflow.start_run(run_name=f\"Session-{CURRENT_DATE.strftime('%Y%m%d_%H%m%S')}\", experiment_id=experiment_id):\n",
    "        for estimator_name, estimator in estimators.items():\n",
    "            with mlflow.start_run(run_name=estimator_name, nested=True, experiment_id=experiment_id): \n",
    "                y_train_pred = estimator.predict(x_train)\n",
    "                y_test_pred = estimator.predict(x_test)\n",
    "\n",
    "                train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "                test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "                # Add the R2 score of the model to the global dict\n",
    "                r2_scores[estimator_name] = test_metrics['r2']\n",
    "\n",
    "                # Log the regressor parameters\n",
    "                mlflow.log_params(estimator.regressor.steps[-1][1].get_params())\n",
    "\n",
    "                # Log the best metric\n",
    "                mlflow.log_metrics(test_metrics)\n",
    "\n",
    "                # Log the model\n",
    "                mlflow.sklearn.log_model(estimator.best_estimator_, \"model\")\n",
    "\n",
    "                logger.info(f\"\"\"{estimator_name} performance \\n{pd.DataFrame({'train': train_metrics, 'test': test_metrics}).T}\"\"\")\n",
    "    return max(r2_scores.items(), key=lambda item: item[1])\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "best_estimator, score = evaluate_models(models, x_train, x_test, y_train, y_test)\n",
    "\n",
    "logger.info(f\"\"\"{best_estimator} is the best estimator found for this problem with an R2 score of {score}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_error_plot(models, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot(models, x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
