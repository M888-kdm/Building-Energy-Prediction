{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from pycaret import regression\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "from src.utils import configure_logger\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(os.path.join(str(Path.cwd().parent), \"src\"))\n",
    "from src.evaluate import evaluate_models\n",
    "from src.plot import prediction_error_plot, residual_plot\n",
    "from settings.params import *\n",
    "from src.tuning import fine_tune_models\n",
    "from src.azure_ml import get_mlflow_tracking_uri\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "configure_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CLEANED_DATA)\n",
    "TARGET_NAME = MODEL_PARAMS['TARGET_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow_uri = get_mlflow_tracking_uri()\n",
    "mlflow.set_tracking_uri(uri=mlflow_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = data.drop(TARGET_NAME, axis=1).select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_data = data.loc[:, numerical_features]\n",
    "\n",
    "num_cols = 3\n",
    "num_rows = (len(numerical_features) + num_cols - 1) // num_cols  # calculate the number of rows needed\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*6, num_rows*5))  # adjust figsize as needed\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    row = i // num_cols\n",
    "    col_pos = i % num_cols\n",
    "    sns.histplot(numerical_data[col], kde=False, ax=axes[row, col_pos])\n",
    "    axes[row, col_pos].set_title(col)\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(numerical_features), len(axes.flat)):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All five distributions in the image appear to be skewed. We are then going to use log transforms on the numerical features in the preprocessing pipeline for the model in order to decrease the skewness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(TARGET_NAME, axis=1), data[TARGET_NAME], test_size=MODEL_PARAMS[\"TEST_SIZE\"], random_state=SEED)\n",
    "\n",
    "logger.info(f\"\\nX train: {x_train.shape}\\nY train: {y_train.shape}\\n\"\n",
    "            f\"X test: {x_test.shape}\\nY test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_train.copy()\n",
    "df[TARGET_NAME] = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_reg = regression.setup(df, target=TARGET_NAME, max_encoding_ohe=200, log_experiment=True, experiment_name=\"building-energy-prediction-training\", train_size=0.8)\n",
    "regression.set_config('seed', SEED)\n",
    "\n",
    "# Removing useless metrics improve training speed\n",
    "regression.remove_metric('MAPE')\n",
    "regression.remove_metric('MSE')\n",
    "regression.remove_metric('RMSLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threes_model = regression.compare_models(n_select=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on multiple types of models with PyCaret shows that for this problem, the models: ExtraTreesRegressor, XGBRegressor, and RandomForestRegressor are the most suitable. However, we believe that the performance obtained during this initial training can be significantly improved. We will wait to do parameter tuning and obtain the final models before making a choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATOR_PARAMS = {\n",
    "    ExtraTreesRegressor.__name__: {\n",
    "        \"estimator\": ExtraTreesRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        \"estimator\": RandomForestRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    },\n",
    "    xgb.XGBRegressor.__name__: {\n",
    "        \"estimator\": xgb.XGBRegressor(),\n",
    "        \"params\": {\n",
    "            'regressor__estimator__n_estimators': np.arange(10, 200, 5),\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cvs = fine_tune_models(ESTIMATOR_PARAMS, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to evaluate the fine-tuned models to see which one we are going to pick as the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining best_models after fine-tuning\n",
    "models = { f\"{estimator_name}\": search_cv.best_estimator_ for estimator_name, search_cv in search_cvs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimator found after evaluation\n",
    "best_estimator, score = evaluate_models(models, x_train, x_test, y_train, y_test)\n",
    "logger.info(f\"\"\"{best_estimator} is the best estimator found for this problem with an R2 score of {score}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluation, we arrive at the conclusion that the best model for this problem is the **Extra Trees Regressor**. Before deploying the model, the best parameters will be picked for the model registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_error_plot(models, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot(models, x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
